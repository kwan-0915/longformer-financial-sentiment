{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a46cdd-659f-4dfb-802d-5d6594431322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRunning\u001b[0m \u001b[33m\u001b[1m$ pipenv lock\u001b[0m \u001b[1mthen\u001b[0m \u001b[33m\u001b[1m$ pipenv sync\u001b[0m\u001b[1m.\u001b[0m\n",
      "Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[KBuilding requirements...\n",
      "\u001b[KResolving dependencies...\n",
      "\u001b[K\u001b[?25h\u001b[32m\u001b[22m✔ Success!\u001b[39m\u001b[22m\u001b[0m \n",
      "Locking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[1mUpdated Pipfile.lock (a1330a3aeb73384a465f4cb9aa3e5bbb7b75c5dfb122226128053882dccc28d6)!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock (cc28d6)...\u001b[0m\n",
      "\u001b[32mAll dependencies are now up-to-date!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pipenv update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868b4271-9e78-4f31-8764-6b40effeaf44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[tf-cpu]\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from transformers[tf-cpu]) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from transformers[tf-cpu]) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from transformers[tf-cpu]) (21.3)\n",
      "Requirement already satisfied: filelock in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from transformers[tf-cpu]) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from transformers[tf-cpu]) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from transformers[tf-cpu]) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from transformers[tf-cpu]) (1.21.5)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp39-cp39-macosx_10_11_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tf2onnx\n",
      "  Downloading tf2onnx-1.12.1-py3-none-any.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.2/442.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.10.0-cp39-cp39-macosx_10_9_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-cpu>=2.3\n",
      "  Downloading tensorflow_cpu-2.10.0-cp39-cp39-macosx_10_14_x86_64.whl (241.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting onnxconverter-common\n",
      "  Downloading onnxconverter_common-1.12.2-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers[tf-cpu]) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers[tf-cpu]) (3.0.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (61.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (14.0.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (2.10.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (22.9.24)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (2.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.42.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (3.6.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (3.19.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.27.0)\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.12.0-cp39-cp39-macosx_10_12_x86_64.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[tf-cpu]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[tf-cpu]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[tf-cpu]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[tf-cpu]) (3.3)\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.11,>=2.10.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-text->transformers[tf-cpu]) (2.10.0)\n",
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.12.0-py3-none-any.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tf2onnx-1.11.1-py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.6/440.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tf2onnx-1.10.1-py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.2/440.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tf2onnx-1.9.3-py3-none-any.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tf2onnx-1.9.2-py3-none-any.whl (430 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m430.2/430.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tf2onnx-1.9.1-py3-none-any.whl (398 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.0/399.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tf2onnx-1.8.5-py3-none-any.whl (370 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.2/370.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading tf2onnx-1.8.4-py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.3/345.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.33.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-cpu>=2.3->transformers[tf-cpu]) (3.2.1)\n",
      "Installing collected packages: tokenizers, tensorflow-hub, onnx, tf2onnx, onnxconverter-common, huggingface-hub, transformers, tensorflow-cpu, tensorflow-text\n",
      "Successfully installed huggingface-hub-0.10.1 onnx-1.12.0 onnxconverter-common-1.12.2 tensorflow-cpu-2.10.0 tensorflow-hub-0.12.0 tensorflow-text-2.10.0 tf2onnx-1.8.4 tokenizers-0.13.1 transformers-4.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[tf-cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17389df-2c15-45f3-a72f-f37ecdea8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11bb14c4-cb8a-4aad-b5b9-57da2ce8e947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 15:32:00.036387: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerTokenizer, TFLongformerForSequenceClassification\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7381ad0-9174-4cfd-9428-5569d341f2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:tokenizer=PreTrainedTokenizer(name_or_path='allenai/longformer-base-4096', vocab_size=50265, model_max_len=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "2022-10-26 19:05:09.199377: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:root:model=<transformers.models.longformer.modeling_tf_longformer.TFLongformerForSequenceClassification object at 0x7f77d1db71f0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_longformer_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " longformer (TFLongformerMai  multiple                 148068864 \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " classifier (TFLongformerCla  multiple                 592130    \n",
      " ssificationHead)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,660,994\n",
      "Trainable params: 148,660,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:None\n",
      "INFO:root:input={'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[   0,  713,   16,   41, 1246, 2788,    2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "INFO:root:outputs=TFLongformerSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.00116083, -0.00183166]], dtype=float32)>, hidden_states=None, attentions=None, global_attentions=None)\n",
      "INFO:root:loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.00116083, -0.00183166]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "longformer_path = \"allenai/longformer-base-4096\"\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(longformer_path)\n",
    "logging.info(f\"{tokenizer=}\")\n",
    "\n",
    "model = TFLongformerForSequenceClassification.from_pretrained(longformer_path)\n",
    "logging.info(f\"{model=}\")\n",
    "logging.info(f\"{model.summary()}\")\n",
    "\n",
    "input = tokenizer(\"This is an example text\", return_tensors=\"tf\")\n",
    "\n",
    "logging.info(f\"{input=}\")\n",
    "\n",
    "outputs = model(input)\n",
    "\n",
    "logging.info(f\"{outputs=}\")\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "logging.info(f\"{loss=}, {logits=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60b6f52a-351a-449d-bd9d-752b2192fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.9/441.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Using cached pyarrow-9.0.0-cp39-cp39-macosx_10_13_x86_64.whl (24.0 MB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: packaging in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/simondanielsson/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, dill, responses, multiprocess, datasets\n",
      "Successfully installed datasets-2.6.1 dill-0.3.5.1 multiprocess-0.70.13 pyarrow-9.0.0 responses-0.18.0 xxhash-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691ab828-5613-47a3-99fa-8e73903816c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset financial_phrasebank (/Users/simondanielsson/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038499645bab418eb9fe02962cde3041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset financial_phrasebank (/Users/simondanielsson/.cache/huggingface/datasets/financial_phrasebank/sentences_75agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b5f2398ca4484fa6a5e296adfa5993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset financial_phrasebank (/Users/simondanielsson/.cache/huggingface/datasets/financial_phrasebank/sentences_66agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d59c5b19d9f4537a2a1d7364df42fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset financial_phrasebank (/Users/simondanielsson/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8af753803594986995d080797828b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence', 'label'],\n",
       "         num_rows: 2264\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence', 'label'],\n",
       "         num_rows: 3453\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence', 'label'],\n",
       "         num_rows: 4217\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence', 'label'],\n",
       "         num_rows: 4846\n",
       "     })\n",
       " })]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = []\n",
    "config_names = ['sentences_allagree', 'sentences_75agree', 'sentences_66agree', 'sentences_50agree']\n",
    "\n",
    "for config in config_names:\n",
    "    datasets += [load_dataset(\"financial_phrasebank\", config)]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c9cb9c-0ced-4ea6-ba5b-2e1426712ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_info': DatasetInfo(description='The key arguments for the low utilization of statistical techniques in\\nfinancial sentiment analysis have been the difficulty of implementation for\\npractical applications and the lack of high quality training data for building\\nsuch models. Especially in the case of finance and economic texts, annotated\\ncollections are a scarce resource and many are reserved for proprietary use\\nonly. To resolve the missing training data problem, we present a collection of\\n∼ 5000 sentences to establish human-annotated standards for benchmarking\\nalternative modeling techniques.\\n\\nThe objective of the phrase level annotation task was to classify each example\\nsentence into a positive, negative or neutral category by considering only the\\ninformation explicitly available in the given sentence. Since the study is\\nfocused only on financial and economic domains, the annotators were asked to\\nconsider the sentences from the view point of an investor only; i.e. whether\\nthe news may have positive, negative or neutral influence on the stock price.\\nAs a result, sentences which have a sentiment that is not relevant from an\\neconomic or financial perspective are considered neutral.\\n\\nThis release of the financial phrase bank covers a collection of 4840\\nsentences. The selected collection of phrases was annotated by 16 people with\\nadequate background knowledge on financial markets. Three of the annotators\\nwere researchers and the remaining 13 annotators were master’s students at\\nAalto University School of Business with majors primarily in finance,\\naccounting, and economics.\\n\\nGiven the large number of overlapping annotations (5 to 8 annotations per\\nsentence), there are several ways to define a majority vote based gold\\nstandard. To provide an objective comparison, we have formed 4 alternative\\nreference datasets based on the strength of majority agreement: all annotators\\nagree, >=75% of annotators agree, >=66% of annotators agree and >=50% of\\nannotators agree.\\n', citation='@article{Malo2014GoodDO,\\n  title={Good debt or bad debt: Detecting semantic orientations in economic texts},\\n  author={P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},\\n  journal={Journal of the Association for Information Science and Technology},\\n  year={2014},\\n  volume={65}\\n}\\n', homepage='https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news', license='Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License', features={'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'neutral', 'positive'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='financial_phrasebank', config_name='sentences_allagree', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=303371, num_examples=2264, dataset_name='financial_phrasebank')}, download_checksums={'https://huggingface.co/datasets/financial_phrasebank/resolve/main/data/FinancialPhraseBank-v1.0.zip': {'num_bytes': 681890, 'checksum': '0e1a06c4900fdae46091d031068601e3773ba067c7cecb5b0da1dcba5ce989a6'}}, download_size=681890, post_processing_size=None, dataset_size=303371, size_in_bytes=985261),\n",
       " '_split': NamedSplit('train'),\n",
       " '_indexes': {},\n",
       " '_data': MemoryMappedTable\n",
       " sentence: string\n",
       " label: int64\n",
       " ----\n",
       " sentence: [[\"According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\",\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\"In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\",\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\",\"Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\",...,\"Operating result for the 12-month period decreased from the profit of EUR0 .4 m while turnover decreased from EUR5 .6 m , as compared to 2004 .\",\"HELSINKI Thomson Financial - Shares in Cargotec fell sharply in early afternoon trade after the cargo handling group posted a surprise drop in April-June profits , which overshadowed the large number of new orders received during the three months .\",\"LONDON MarketWatch -- Share prices ended lower in London Monday as a rebound in bank stocks failed to offset broader weakness for the FTSE 100 .\",\"Operating profit fell to EUR 35.4 mn from EUR 68.8 mn in 2007 , including vessel sales gain of EUR 12.3 mn .\",\"Sales in Finland decreased by 10.5 % in January , while sales outside Finland dropped by 17 % .\"]]\n",
       " label: [[1,2,2,2,2,...,0,0,0,0,0]],\n",
       " '_indices': None,\n",
       " '_format_type': None,\n",
       " '_format_kwargs': {},\n",
       " '_format_columns': None,\n",
       " '_output_all_columns': False,\n",
       " '_fingerprint': '732f56dfdf674733'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['train'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ef2350-a9ac-492e-a297-e06309629988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'neutral', 'positive'], id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['train'].__dict__['_info'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f20f2a-757b-444b-9d85-b34afc230909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'neutral', 2: 'positive'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label translation\n",
    "available_labels = datasets[0]['train'].__dict__['_info'].features['label'].names\n",
    "idx2label = {index: label for index, label in enumerate(available_labels)}\n",
    "idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f959b1d6-6212-4438-828b-7c1a262d42f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': SplitInfo(name='train', num_bytes=303371, num_examples=2264, dataset_name='financial_phrasebank')}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['train'].__dict__['_info'].splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6199c2d0-151e-416d-94c3-317b7d939fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]['train']['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42d0cca4-a040-40a2-843a-87e496a8042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label[datasets[0]['train']['label'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bd858e-aaa0-4682-a6f1-bddb1201813f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
       "array([[    0, 14693,     7,  9328,  2156,     5,   138,    34,   117,\n",
       "          708,     7,   517,    70,   931,     7,   798,  2156,  1712,\n",
       "           14,    16,   147,     5,   138,    16,  1197,   479,     2]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = datasets[0]['train']['sentence']\n",
    "labels = datasets[0]['train']['label']\n",
    "\n",
    "example_inputs = tokenizer(sentences[0], return_tensors=\"tf\")\n",
    "example_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a71f7e0-4ace-4a0d-a196-6ef1f9f440d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFLongformerSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.01157178, -0.01972083]], dtype=float32)>, hidden_states=None, attentions=None, global_attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "341ef77f-2158-4bd9-b7af-94ad86a0529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62905fdb-d082-4993-9303-ce2c25f793ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED\n",
    "\n",
    "# tokenize data\n",
    "tokenized_data = tokenizer(sentences, return_tensors=\"np\", padding=True)\n",
    "\n",
    "# convert to numpy arrays\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b9a6c47-aaa6-4c03-9cfb-7d7a262186b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[    0, 14693,     7, ...,     1,     1,     1],\n",
       "       [    0,  2709,     5, ...,     1,     1,     1],\n",
       "       [    0,  1121,     5, ...,     1,     1,     1],\n",
       "       ...,\n",
       "       [    0,   574,  4524, ...,     1,     1,     1],\n",
       "       [    0, 20420,  1295, ...,     1,     1,     1],\n",
       "       [    0, 28188,    11, ...,     1,     1,     1]]), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view \n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e689447-a8df-43bb-95b5-1bed2b38fd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 14693,     7,  9328,  2156,     5,   138,    34,   117,\n",
       "         708,     7,   517,    70,   931,     7,   798,  2156,  1712,\n",
       "          14,    16,   147,     5,   138,    16,  1197,   479,     2,\n",
       "           1,     1,     1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['input_ids'][0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b004d1bf-a23e-4a9a-8426-2e0e473d31d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b581f-b1cc-4ce4-8582-2b4f42d4a466",
   "metadata": {},
   "source": [
    "We see that we pad after about 25 words (30 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c1dd63-8ee1-41c8-bb6d-5447956cf9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/simondanielsson/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141/cache-08e0e3abf31d160d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2264\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USE THIS \n",
    "\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"sentence\"])\n",
    "\n",
    "tokenized_dataset = datasets[0]['train'].map(tokenize_dataset)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb4a9f95-c223-403a-a671-a3a4e0e9c06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b60350a-e3c6-4fcb-a128-7a5eae9089be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "sentence: string\n",
       "label: int64\n",
       "input_ids: list<item: int32>\n",
       "  child 0, item: int32\n",
       "attention_mask: list<item: int8>\n",
       "  child 0, item: int8\n",
       "----\n",
       "sentence: [[\"According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\",\"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\"In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\",\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\",\"Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\",...,\"Metsa-Botnia will finance the payment of dividends , the repayment of capital and the repurchase of its own shares with the funds deriving from its divestment of the Uruguay operations , and shares in Pohjolan Voima , and by utilising its existing financing facilities .\",\"Metso 's delivery will include a complete coated board line with related air systems and two winders .\",\"Most of the dividend will go to the Grimaldi family .\",\"Neste Shipping is the most likely to remain Finnish as the oil sector and its transports are significant for emergency supply .\",\"Net investment income\"],[\"no compensation for its news , opinions or distributions .\",\"No financial details were disclosed .\",\"Of the company 's net sales , 38 % was acquired in Finland , 21 % in other European countries , 40 % in Asia , and 1 % in the US .\",\"Officials did not disclose the contract value .\",\"Otherwise the situation is under control .\",...,\"HELSINKI AFX - Salcomp , the mobile phone charger manufacturer , said it has appointed Markku Hangasjarvi as its new CEO , following the resignation of Mats Eriksson .\",\"HK Ruokatalo produces many turkey products , such as cold cuts .\",\"However , the brokers ' ratings on the stock differ .\",\"Mursula said they tried to gather macro-economic perspective to see how Malaysia was doing .\",\"Niklas Skogster has been employed by the ABB Group in various positions concerning the development of operations .\"],[\"The beers differ slightly from mainstream beers .\",\"The company operates a U.S. division in Lisle , Ill. .\",\"The deal covers Stockmann Auto Oy Ab 's sales and after-sales services concerning Volkswagen and Audi in Helsinki , Espoo and Vantaa .\",\"The equipment is designated to Bollore Africa Logistics terminal Societe d'Exploitation du Terminal de Vridi SETV in Abidjan , Ivory Coast and the delivery is scheduled to start in March 2010 .\",\"The floor area of the Yliopistonrinne project will be 7,900 sq m and the building 's gross area will total 12,800 sq m. A total 25.1 % of the facilities have been let .\",...,\"Operating result for the 12-month period decreased from the profit of EUR0 .4 m while turnover decreased from EUR5 .6 m , as compared to 2004 .\",\"HELSINKI Thomson Financial - Shares in Cargotec fell sharply in early afternoon trade after the cargo handling group posted a surprise drop in April-June profits , which overshadowed the large number of new orders received during the three months .\",\"LONDON MarketWatch -- Share prices ended lower in London Monday as a rebound in bank stocks failed to offset broader weakness for the FTSE 100 .\",\"Operating profit fell to EUR 35.4 mn from EUR 68.8 mn in 2007 , including vessel sales gain of EUR 12.3 mn .\",\"Sales in Finland decreased by 10.5 % in January , while sales outside Finland dropped by 17 % .\"]]\n",
       "label: [[1,2,2,2,2,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,0,0,0,0,0]]\n",
       "input_ids: [[[0,14693,7,9328,2156,...,138,16,1197,479,2],[0,2709,5,94,297,...,10353,406,119,479,2],...,[0,487,13967,26466,16,...,13,1923,1787,479,2],[0,15721,915,1425,2]],[[0,2362,4660,13,63,...,5086,50,26070,479,2],[0,3084,613,1254,58,4638,479,2],...,[0,448,4668,5571,26,...,5697,21,608,479,2],[0,38334,15086,4058,2154,...,709,9,1414,479,2]],[[0,133,16328,10356,2829,31,7302,16328,479,2],[0,133,138,4497,10,...,2156,12285,4,479,2],...,[0,20420,1295,1963,1064,...,246,475,282,479,2],[0,28188,11,12587,8065,...,30,601,7606,479,2]]]\n",
       "attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a7630e-c34b-430d-8c0e-4eecb444d2be",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb4af245-a7ae-4b4f-955f-1d5087fefeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFLongformerForSequenceClassification.from_pretrained(longformer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd174db-aa6a-4cf6-a529-143294b3419c",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc21963-cfd3-423f-9117-99eeed083482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = model.prepare_tf_dataset(tokenized_dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86fdf402-73b8-4f4b-9176-da102250bf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(16, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(16, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(16,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f3db918-d719-429b-a11e-e66b2c2487c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=Adam(3e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ffeed-88bf-4d91-86bf-2aebd242a7ff",
   "metadata": {},
   "source": [
    "Inspect the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75bb8193-acff-4a18-9da4-c328b981a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_longformer_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " longformer (TFLongformerMai  multiple                 148068864 \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " classifier (TFLongformerCla  multiple                 592130    \n",
      " ssificationHead)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,660,994\n",
      "Trainable params: 592,130\n",
      "Non-trainable params: 148,068,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06fd1b78-d1ce-4428-b793-0fb1ad6a8c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.longformer.modeling_tf_longformer.TFLongformerMainLayer at 0x7fb649077b20>,\n",
       " <transformers.models.longformer.modeling_tf_longformer.TFLongformerClassificationHead at 0x7fb649077c40>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5af84762-89c3-486f-ae7b-09e98ea318f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze the longformer's weights\n",
    "model.layers[0].trainable = False\n",
    "model.layers[0].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "507ba5a8-e5ff-4f53-9151-fabf2492bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built True\n",
      "dense <keras.layers.core.dense.Dense object at 0x7fb649400820>\n",
      "dropout <keras.layers.regularization.dropout.Dropout object at 0x7fb649400850>\n",
      "out_proj <keras.layers.core.dense.Dense object at 0x7fb649400e80>\n"
     ]
    }
   ],
   "source": [
    "# Inspect the classifier's (public) components\n",
    "for key, value in model.layers[1].__dict__.items():\n",
    "    if not key.startswith('_'):\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea948d-c4e2-4d03-83fd-2c602c7ac624",
   "metadata": {},
   "source": [
    "Take a look at the **dense** layer\n",
    "\n",
    "It uses a tanh activation, and 768 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebd854d7-4d32-48e4-89db-ead232331eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units = 768\n",
      "activation = <function tanh at 0x7fb666fd7ee0>\n",
      "use_bias = True\n",
      "kernel_initializer = <keras.initializers.initializers_v2.TruncatedNormal object at 0x7fb649400730>\n",
      "bias_initializer = <keras.initializers.initializers_v2.Zeros object at 0x7fb6494009d0>\n",
      "kernel_regularizer = None\n",
      "bias_regularizer = None\n",
      "kernel_constraint = None\n",
      "bias_constraint = None\n",
      "kernel = <tf.Variable 'tf_longformer_for_sequence_classification/classifier/dense/kernel:0' shape=(768, 768) dtype=float32, numpy=\n",
      "array([[-0.01040684, -0.00096314, -0.01462177, ..., -0.01802972,\n",
      "         0.01053835,  0.01930583],\n",
      "       [ 0.00552768, -0.01565199, -0.00712103, ..., -0.03049491,\n",
      "         0.01024438,  0.00122795],\n",
      "       [-0.01091987,  0.0288004 , -0.01887974, ..., -0.02267739,\n",
      "        -0.00482497,  0.02033021],\n",
      "       ...,\n",
      "       [ 0.03318232,  0.00952725, -0.00161686, ..., -0.03183749,\n",
      "         0.02155213, -0.0297372 ],\n",
      "       [-0.00433407, -0.00770387, -0.02269859, ...,  0.01136418,\n",
      "        -0.01272887, -0.03979503],\n",
      "       [-0.02309848, -0.00753874, -0.03236273, ...,  0.00321131,\n",
      "         0.01430841, -0.00072457]], dtype=float32)>\n",
      "bias = <tf.Variable 'tf_longformer_for_sequence_classification/classifier/dense/bias:0' shape=(768,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>\n",
      "built = True\n"
     ]
    }
   ],
   "source": [
    "def print_public_attrs(_dict: dict) -> None:\n",
    "    for key, value in _dict.items():\n",
    "        if not key.startswith('_'):\n",
    "            print(key, '=', value)\n",
    "\n",
    "print_public_attrs(model.layers[1].dense.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45438a70-7402-4d7c-a6ee-62ad388bb2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units = 2\n",
      "activation = <function linear at 0x7fb666fdb5e0>\n",
      "use_bias = True\n",
      "kernel_initializer = <keras.initializers.initializers_v2.TruncatedNormal object at 0x7fb649400ca0>\n",
      "bias_initializer = <keras.initializers.initializers_v2.Zeros object at 0x7fb649434070>\n",
      "kernel_regularizer = None\n",
      "bias_regularizer = None\n",
      "kernel_constraint = None\n",
      "bias_constraint = None\n",
      "kernel = <tf.Variable 'tf_longformer_for_sequence_classification/classifier/out_proj/kernel:0' shape=(768, 2) dtype=float32, numpy=\n",
      "array([[ 0.01424338,  0.00368631],\n",
      "       [ 0.03687448, -0.01114013],\n",
      "       [ 0.03021111, -0.03322965],\n",
      "       ...,\n",
      "       [-0.00972675, -0.01798492],\n",
      "       [ 0.00853285,  0.0016353 ],\n",
      "       [ 0.02118233, -0.02248817]], dtype=float32)>\n",
      "bias = <tf.Variable 'tf_longformer_for_sequence_classification/classifier/out_proj/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n",
      "built = True\n"
     ]
    }
   ],
   "source": [
    "print_public_attrs(model.layers[1].out_proj.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a53cbff7-88a6-4b69-b9b8-817d1461ba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built = False\n",
      "units = 3\n",
      "activation = <function softmax at 0x7fb666fd7160>\n",
      "use_bias = True\n",
      "kernel_initializer = <keras.initializers.initializers_v2.GlorotUniform object at 0x7fb5e20c6130>\n",
      "bias_initializer = <keras.initializers.initializers_v2.Zeros object at 0x7fb5e1fb8f40>\n",
      "kernel_regularizer = None\n",
      "bias_regularizer = None\n",
      "kernel_constraint = None\n",
      "bias_constraint = None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "new_dense = Dense(units=3, activation=\"softmax\")\n",
    "\n",
    "print_public_attrs(new_dense.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8563a0a7-2fb4-46a2-b6ba-24bd2ea7330d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.layers.core.dense.Dense"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.layers[1].out_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf5c4c-6c8b-4513-9a90-5f2172b8ba90",
   "metadata": {},
   "source": [
    "Let's modify the last layer to accomodate 3 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21f3797e-66b5-47d5-b651-8a751bf9fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built = False\n",
      "units = 3\n",
      "activation = <function softmax at 0x7fb666fd7160>\n",
      "use_bias = True\n",
      "kernel_initializer = <keras.initializers.initializers_v2.GlorotUniform object at 0x7fb5e20c6130>\n",
      "bias_initializer = <keras.initializers.initializers_v2.Zeros object at 0x7fb5e1fb8f40>\n",
      "kernel_regularizer = None\n",
      "bias_regularizer = None\n",
      "kernel_constraint = None\n",
      "bias_constraint = None\n"
     ]
    }
   ],
   "source": [
    "model.layers[1].out_proj = new_dense\n",
    "print_public_attrs(model.layers[1].out_proj.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d6994a2-7a9e-4e3b-8d87-54433cc7c553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFLongformerSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.41237852, 0.3686567 , 0.21896479]], dtype=float32)>, hidden_states=None, attentions=None, global_attentions=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(example_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662edc6c-48ee-4360-9fce-21011e79a958",
   "metadata": {},
   "source": [
    "Let's create a new classification head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d4779bc-d98e-44f5-927e-9afe04e9f9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_self_setattr_tracking': True,\n",
       " '_instrumented_keras_api': True,\n",
       " '_instrumented_keras_layer_class': True,\n",
       " '_instrumented_keras_model_class': False,\n",
       " '_stateful': False,\n",
       " 'built': True,\n",
       " '_input_spec': None,\n",
       " '_build_input_shape': TensorShape([3, 5]),\n",
       " '_saved_model_inputs_spec': TensorSpec(shape=(3, 5), dtype=tf.int64, name=None),\n",
       " '_saved_model_arg_spec': ([TensorSpec(shape=(3, 5), dtype=tf.int64, name=None)],\n",
       "  {'attention_mask': TensorSpec(shape=(3, 5), dtype=tf.int64, name=None),\n",
       "   'global_attention_mask': TensorSpec(shape=(3, 5), dtype=tf.int64, name=None)}),\n",
       " '_supports_masking': False,\n",
       " '_name': 'longformer',\n",
       " '_activity_regularizer': None,\n",
       " '_trainable_weights': [],\n",
       " '_non_trainable_weights': [],\n",
       " '_updates': [],\n",
       " '_thread_local': <_thread._local at 0x7fb64a1739f0>,\n",
       " '_callable_losses': [],\n",
       " '_losses': [],\n",
       " '_metrics': [],\n",
       " '_metrics_lock': <unlocked _thread.lock object at 0x7fb649077fc0>,\n",
       " '_dtype_policy': <Policy \"float32\">,\n",
       " '_compute_dtype_object': tf.float32,\n",
       " '_autocast': True,\n",
       " '_self_tracked_trackables': [ListWrapper([512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]),\n",
       "  <transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings at 0x7fb64a176100>,\n",
       "  <transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder at 0x7fb64a176940>,\n",
       "  {'add_pooling_layer': False, 'name': 'longformer'}],\n",
       " '_inbound_nodes_value': [],\n",
       " '_outbound_nodes_value': [],\n",
       " '_call_spec': <keras.utils.layer_utils.CallFunctionSpec at 0x7fb64a176070>,\n",
       " '_dynamic': False,\n",
       " '_initial_weights': None,\n",
       " '_auto_track_sub_layers': True,\n",
       " '_preserve_input_structure_in_config': False,\n",
       " '_name_scope_on_declaration': '',\n",
       " '_captured_weight_regularizer': [],\n",
       " '_obj_reference_counts_dict': ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping LongformerConfig {\n",
       "   \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
       "   \"attention_mode\": \"longformer\",\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"attention_window\": [\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512\n",
       "   ],\n",
       "   \"bos_token_id\": 0,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"ignore_attention_mask\": false,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-05,\n",
       "   \"max_position_embeddings\": 4098,\n",
       "   \"model_type\": \"longformer\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"onnx_export\": false,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"sep_token_id\": 2,\n",
       "   \"transformers_version\": \"4.23.1\",\n",
       "   \"type_vocab_size\": 1,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50265\n",
       " }\n",
       " >: 2, <_ObjectIdentityWrapper wrapping 12>: 1, <_ObjectIdentityWrapper wrapping 0.02>: 1, <_ObjectIdentityWrapper wrapping False>: 3, <_ObjectIdentityWrapper wrapping 1>: 1, <_ObjectIdentityWrapper wrapping ListWrapper([512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512])>: 1, <_ObjectIdentityWrapper wrapping <transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings object at 0x7fb64a176100>>: 1, <_ObjectIdentityWrapper wrapping <transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder object at 0x7fb64a176940>>: 1, <_ObjectIdentityWrapper wrapping DictWrapper({'add_pooling_layer': False, 'name': 'longformer'})>: 1}),\n",
       " 'config': LongformerConfig {\n",
       "   \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
       "   \"attention_mode\": \"longformer\",\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"attention_window\": [\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512\n",
       "   ],\n",
       "   \"bos_token_id\": 0,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"ignore_attention_mask\": false,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-05,\n",
       "   \"max_position_embeddings\": 4098,\n",
       "   \"model_type\": \"longformer\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"onnx_export\": false,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"sep_token_id\": 2,\n",
       "   \"transformers_version\": \"4.23.1\",\n",
       "   \"type_vocab_size\": 1,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50265\n",
       " },\n",
       " 'num_hidden_layers': 12,\n",
       " 'initializer_range': 0.02,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'return_dict': True,\n",
       " 'pad_token_id': 1,\n",
       " '_self_unconditional_checkpoint_dependencies': [TrackableReference(name=attention_window, ref=ListWrapper([512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512])),\n",
       "  TrackableReference(name=embeddings, ref=<transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings object at 0x7fb64a176100>),\n",
       "  TrackableReference(name=encoder, ref=<transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder object at 0x7fb64a176940>),\n",
       "  TrackableReference(name=_kwargs, ref={'add_pooling_layer': False, 'name': 'longformer'})],\n",
       " '_self_unconditional_dependency_names': {'attention_window': ListWrapper([512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]),\n",
       "  'embeddings': <transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings at 0x7fb64a176100>,\n",
       "  'encoder': <transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder at 0x7fb64a176940>,\n",
       "  '_kwargs': {'add_pooling_layer': False, 'name': 'longformer'}},\n",
       " '_self_unconditional_deferred_dependencies': {},\n",
       " '_self_update_uid': -1,\n",
       " '_self_name_based_restores': set(),\n",
       " '_self_saveable_object_factories': {},\n",
       " 'attention_window': ListWrapper([512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]),\n",
       " 'embeddings': <transformers.models.longformer.modeling_tf_longformer.TFLongformerEmbeddings at 0x7fb64a176100>,\n",
       " 'encoder': <transformers.models.longformer.modeling_tf_longformer.TFLongformerEncoder at 0x7fb64a176940>,\n",
       " 'pooler': None,\n",
       " '_config': LongformerConfig {\n",
       "   \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
       "   \"attention_mode\": \"longformer\",\n",
       "   \"attention_probs_dropout_prob\": 0.1,\n",
       "   \"attention_window\": [\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512,\n",
       "     512\n",
       "   ],\n",
       "   \"bos_token_id\": 0,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"hidden_size\": 768,\n",
       "   \"ignore_attention_mask\": false,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 3072,\n",
       "   \"layer_norm_eps\": 1e-05,\n",
       "   \"max_position_embeddings\": 4098,\n",
       "   \"model_type\": \"longformer\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"onnx_export\": false,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"sep_token_id\": 2,\n",
       "   \"transformers_version\": \"4.23.1\",\n",
       "   \"type_vocab_size\": 1,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50265\n",
       " },\n",
       " '_kwargs': {'add_pooling_layer': False, 'name': 'longformer'},\n",
       " '_trainable': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77966860-28fc-40fa-b002-7350df5bb1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
       "array([[    0, 14693,     7,  9328,  2156,     5,   138,    34,   117,\n",
       "          708,     7,   517,    70,   931,     7,   798,  2156,  1712,\n",
       "           14,    16,   147,     5,   138,    16,  1197,   479,     2]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b24f3f5-8cc7-48eb-a97f-0992c8f75f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 27])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e9eabc5-9cc9-4d3b-911d-ae72af3e0e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFLongformerBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(1, 27, 768), dtype=float32, numpy=\n",
       "array([[[-4.0485293e-02,  8.6553887e-02, -3.6896270e-02, ...,\n",
       "         -1.6384226e-01, -4.6622265e-02, -9.9837789e-03],\n",
       "        [-5.3308029e-03,  1.3362442e-01,  8.5438408e-02, ...,\n",
       "         -4.4666767e-01, -2.4286658e-04,  3.0768421e-02],\n",
       "        [ 8.0907546e-02,  1.9714523e-02,  3.1975619e-03, ...,\n",
       "         -7.9686332e-01, -1.2787409e-01, -5.6903429e-02],\n",
       "        ...,\n",
       "        [-1.6714707e-02,  1.6344190e-02,  3.9270516e-02, ...,\n",
       "         -2.7256891e-02,  1.1115223e-03,  8.2088426e-02],\n",
       "        [ 1.1323520e-01, -3.0629668e-01,  1.8324254e-01, ...,\n",
       "         -8.8297002e-02, -1.8919486e-01, -4.8807099e-02],\n",
       "        [-3.9318725e-02,  8.9037821e-02, -5.0074685e-02, ...,\n",
       "         -1.8340765e-01, -4.0848423e-02, -1.8299578e-02]]], dtype=float32)>, pooler_output=None, hidden_states=None, attentions=None, global_attentions=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0](example_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d6ec1-04b9-487b-bb14-e1a47ffbaaf9",
   "metadata": {},
   "source": [
    "We see that the Longformer output is of shape `(batch_size, sequence_length, output_embedding_dim)`, where `output_embedding_dim = 768`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11de2f-3cff-4e77-9a91-eedbbee1b844",
   "metadata": {},
   "source": [
    "Let's create a new model using longformer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be72b041-3940-462b-b3a8-54b650a93c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2264\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a83476-cdf1-42ec-a390-59b87af282c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_longformer_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " longformer (TFLongformerMai  multiple                 148068864 \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " classifier (TFLongformerCla  multiple                 592130    \n",
      " ssificationHead)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,660,994\n",
      "Trainable params: 592,130\n",
      "Non-trainable params: 148,068,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4dfba3-5dbe-462a-817e-bb7b7eed1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_path = \"/Users/simondanielsson/Documents/Theses/longformer-sentiment/data/totalEURUSDnews.xlsx\"\n",
    "\n",
    "news = pd.read_excel(news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0bd3c3-4609-4842-a0d0-d65be598f7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EUR/USD pulls back on Friday, still heads for ...</td>\n",
       "      <td>US dollar recovers ground as Euro gets hits by...</td>\n",
       "      <td>2018-09-21T16:34:04Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EUR/USD: Fed should help for another visit to ...</td>\n",
       "      <td>Next week, the Federal Reserve will meet. Acco...</td>\n",
       "      <td>2018-09-21T17:41:48Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EUR/USD: Wider US-DE yield spreads and risk-of...</td>\n",
       "      <td>The EUR/USD fell on Friday, tracking the 10-ye...</td>\n",
       "      <td>2018-09-24T03:44:36Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forex Today: A hectic weeks kicks off with a s...</td>\n",
       "      <td>Markets opened quietly this week, belying the ...</td>\n",
       "      <td>2018-09-24T05:19:56Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EUR/USD probing lows near 1.1720 ahead of IFO</td>\n",
       "      <td>The pair has started the week on a soft footin...</td>\n",
       "      <td>2018-09-24T07:02:33Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7408</th>\n",
       "      <td>Forex Today: Dollar rebounds as markets turn c...</td>\n",
       "      <td>Here is what you need to know on Tuesday, May ...</td>\n",
       "      <td>2021-05-04 06:14:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7409</th>\n",
       "      <td>EUR/USD Forecast: Bear attack on 1.20 looks im...</td>\n",
       "      <td>EUR/USD has dropped off the highs, surrenderin...</td>\n",
       "      <td>2021-05-04 06:40:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7410</th>\n",
       "      <td>EUR/USD to fall below 1.20 amid concerns about...</td>\n",
       "      <td>EUR/USD has dropped off the highs, surrenderin...</td>\n",
       "      <td>2021-05-04 06:49:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7411</th>\n",
       "      <td>EUR/USD bearish breakout starts ABC zigzag pat...</td>\n",
       "      <td>EUR/USD broke the support trend line (dotted g...</td>\n",
       "      <td>2021-05-04 06:57:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7412</th>\n",
       "      <td>BoE meeting and Scottish parliamentary electio...</td>\n",
       "      <td>Markets Let&amp;rsquo;s pick up right where we lef...</td>\n",
       "      <td>2021-05-04 07:02:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7413 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     EUR/USD pulls back on Friday, still heads for ...   \n",
       "1     EUR/USD: Fed should help for another visit to ...   \n",
       "2     EUR/USD: Wider US-DE yield spreads and risk-of...   \n",
       "3     Forex Today: A hectic weeks kicks off with a s...   \n",
       "4         EUR/USD probing lows near 1.1720 ahead of IFO   \n",
       "...                                                 ...   \n",
       "7408  Forex Today: Dollar rebounds as markets turn c...   \n",
       "7409  EUR/USD Forecast: Bear attack on 1.20 looks im...   \n",
       "7410  EUR/USD to fall below 1.20 amid concerns about...   \n",
       "7411  EUR/USD bearish breakout starts ABC zigzag pat...   \n",
       "7412  BoE meeting and Scottish parliamentary electio...   \n",
       "\n",
       "                                            articleBody                  Date  \n",
       "0     US dollar recovers ground as Euro gets hits by...  2018-09-21T16:34:04Z  \n",
       "1     Next week, the Federal Reserve will meet. Acco...  2018-09-21T17:41:48Z  \n",
       "2     The EUR/USD fell on Friday, tracking the 10-ye...  2018-09-24T03:44:36Z  \n",
       "3     Markets opened quietly this week, belying the ...  2018-09-24T05:19:56Z  \n",
       "4     The pair has started the week on a soft footin...  2018-09-24T07:02:33Z  \n",
       "...                                                 ...                   ...  \n",
       "7408  Here is what you need to know on Tuesday, May ...   2021-05-04 06:14:46  \n",
       "7409  EUR/USD has dropped off the highs, surrenderin...   2021-05-04 06:40:30  \n",
       "7410  EUR/USD has dropped off the highs, surrenderin...   2021-05-04 06:49:00  \n",
       "7411  EUR/USD broke the support trend line (dotted g...   2021-05-04 06:57:29  \n",
       "7412  Markets Let&rsquo;s pick up right where we lef...   2021-05-04 07:02:36  \n",
       "\n",
       "[7413 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952655fd-56c6-4e98-9338-ab7cfc42f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "longformer_path = \"allenai/longformer-base-4096\"\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(longformer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fd709ec-6577-4c06-9683-1d961eccaaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': <tf.Tensor: shape=(1, 388), dtype=int32, numpy=\n",
       " array([[    0,  3048,  1404, 29283,  1255,    25,  5122,  1516,  2323,\n",
       "            30,  2404,  1379,     4, 10353,    73, 14505,   202,  3885,\n",
       "            13,  4114,  3077,  1135,   273,   947,  4926,  2253,   139,\n",
       "           131,    29,  7117,     4,    20, 10353,    73, 14505,  1763,\n",
       "          2468,   124,    31,     5,  1609,   672,   187,   502,     8,\n",
       "         20856,   233,     9,  2350,   947,  4926,  2253,   139,   131,\n",
       "            29,  3077,     4,    83,  6473,  2752,     9,     5,   382,\n",
       "          1404,     8,   987,  2784,   392,  1450,    15,  2404,   478,\n",
       "             5,  5122,     4,  3322,   452,     6,     5,  1763,  3112,\n",
       "             5,  2669,     8,  1348,   112,     4,  1366,  4197,     4,\n",
       "            85,   685,  4093,   198,     5,   112,     4, 38510,     8,\n",
       "          2468,   124,  6473,   352,     4,   572,   987,  2784,   392,\n",
       "          6075,  2098,    31,  1281,   917,     8,  1179,  1379,    59,\n",
       "            10,   359,  4779,  2253,   139,   131,  2362,   432,   947,\n",
       "          2586,  2253,   139,   131,  4258,    15,  2404,     6, 10353,\n",
       "            73, 14505,  1882,   511,     5,  5386,  7117,     9,  7216,\n",
       "           510,    73, 14505,     4,   572,     5,  1786,     9,     5,\n",
       "           382,  1852,     6,     5,  2287,  2576,   196,    23,   112,\n",
       "             4,  1360,   541,     4,  1590,     5,    94,   722,    24,\n",
       "            56,    57,  1375, 29445,    11,     5,   112,     4,  1360,\n",
       "          3506,    73,   134,     4,  1360,   541,  1186,     6,   159,\n",
       "            13,     5,   183,     6,    53,   202,    55,    87,    10,\n",
       "          6317,   181,  7418,  1065,     5,   672,    24,    56,    10,\n",
       "           186,   536,     4,    85,    16,    59,     7,   618,     5,\n",
       "          1609,  4114,   593,   187,   502,     4,  7830,  3077,    58,\n",
       "          2800,    30,    10,  8074,   382,  1404,     4,  4130,   186,\n",
       "             5,   762,   515,    40,    28,     5,  2337,   947,  4926,\n",
       "          2253,   139,   131,    29,   529,     4,    83,   731,  5960,\n",
       "            16,   421,     8,     5,  6328,     8,  8979,     9,     5,\n",
       "           274,  3765,   347,   813,    32,   533,     7,   278,     5,\n",
       "          6328,     9,     5,   382,  1404,     4, 41432,     7,  1183,\n",
       "            20,  2669,     9, 10353,    73, 14505, 12428,    15,   273,\n",
       "             6,    53,     5,  2904,   202,   332,     7,     5,  7237,\n",
       "             4,   287,   251,    25,    24,  1189,  1065,    41, 18256,\n",
       "         10082,   516,    31,   830,  6917,     6,   855,    23,   112,\n",
       "             4,  1549,  1749,     6,     5, 12115,  2904,    40,  1091,\n",
       "            11,   317,     4,  3224,    14,   516,     6,   323,    16,\n",
       "           450,    23,   112,     4,  1360,   844,     8,   112,     4,\n",
       "          1360,   612,     4,   598,     5,  7237,     6,     5,  3169,\n",
       "          5910,    16,   450,    23,   112,     4,  1360,  3506,    73,\n",
       "          3083,     8,   172,     5,   112,     4, 38510,    73,  2546,\n",
       "           443,     4,    83,  1108,   723,   115,   699,     5,   169,\n",
       "            13,    10,  1296,     9,   112,     4,  1366,  1096,     4,\n",
       "             2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 388), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>},\n",
       " {'input_ids': <tf.Tensor: shape=(1, 258), dtype=int32, numpy=\n",
       " array([[    0, 19192,   186,     6,     5,  1853,  3965,    40,   972,\n",
       "             4,   767,     7,  1066,    31,   211,  1253,  1071,   788,\n",
       "             6,  1143,  2337, 10204,   197,   244, 10353,    73, 14505,\n",
       "         23222,     5,   112,     4,   996,   443,   456,   148,     5,\n",
       "           768,     9,     5, 13270,     4,  4300,  3232, 17467,    35,\n",
       "           359,  4779,  2253,   139,   131,  3908,     5,  2337,   278,\n",
       "             7,  1095,    15, 23434, 20366,    13,   122,     6,   382,\n",
       "          1162,    32,   278,     7,  1095,    10,  1300,     9,  6775,\n",
       "           323,     4,   152,   197,   244, 10546,     5,  2194,     9,\n",
       "             5,  1404,    25,    10,  2324,  2593,   258,    11,  1110,\n",
       "             9,     5,   672,     9,     8,     5,   464,    11,   765,\n",
       "            12,  1397,  5167,     4,   947,  2586,  2253,   139,   131,\n",
       "           359,  4779,  2253,   139,   131,  3908,     5,  2337,   202,\n",
       "          5609,     7,   535,     5,   609,     9,  1375,  1162,   124,\n",
       "          1567,   359,  6634,  2253,   139,   131, 12516,   947,  4926,\n",
       "          2253,   139,   131,     6,    24,  1189,   350,   419,    11,\n",
       "            84,  1217,    13,     5, 13175,   210,     7,   425,     5,\n",
       "          2337,   164,    15,   946,     4,   152,   197,   244, 10353,\n",
       "            73, 14505, 23222,     5,   112,     4,   996,   443,   456,\n",
       "           148,     5,   768,     9,     5, 13270,     4,   947,  2586,\n",
       "          2253,   139,   131,   359,  4779,  2253,   139,   131,  1620,\n",
       "             5,  6899,    16,   278,     7,  6029,    10,    78,  5960,\n",
       "           567,    62,    23,    10,    86,   147,     5,  2337,   115,\n",
       "            28,   546,     7,   213,    15,   946,     6,    10, 10353,\n",
       "            73, 14505, 20646,    40,   386,     7,  1468,  1496,     4,\n",
       "          7908,     6,    24,    16,    77,  8673,  6897,   359,  1187,\n",
       "          1671,   131,  1195,    87,    77, 10204,  5948,   111,    14,\n",
       "          2593,  9819,    16,   450,     6,     8,  2626, 28550,     4,\n",
       "           947,  2586,  2253,   139,   131,     2]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 258), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = news.loc[0, 'articleBody']\n",
    "tokenized_text = [tokenizer(news.loc[i, 'articleBody'], return_tensors=\"tf\") for i in range(2)]\n",
    "\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43e2916c-4142-4f17-8f26-7dae5b5049d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(2,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_data = tf.data.Dataset.from_tensor_slices(tokenized_text)\n",
    "\n",
    "inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad85d29f-14ab-4a70-bd45-8749adb52fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFLongformerForSequenceClassification.from_pretrained(longformer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c89b3cf6-bf54-469f-af96-152b01c6e4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.03143824, 0.12860347]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f1d706ad-d0c6-47d9-ad5e-3ad098160af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "da5c8874-b352-4f2e-92fe-37e70ee9c541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simondanielsson/Documents/Theses/longformer-sentiment/exploration'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9aadf258-f6c2-4dcc-980d-137da9302a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def populate_articlebody_sentiment(_news, model, tokenizer):\n",
    "    articleBody_sentiment_columns = ['articleBody_negative', 'articleBody_neutral', 'articleBody_positive']\n",
    "\n",
    "    for index, val in enumerate(_news['articleBody']):\n",
    "        # since we only do this once we don't bother creating batches\n",
    "        \n",
    "        tokenized_text = tokenizer(val, return_tensors=\"tf\")\n",
    "\n",
    "        output = model(**tokenized_text)\n",
    "\n",
    "        probs_np = tf.nn.softmax(output.logits).numpy()[0]\n",
    "\n",
    "        _news.loc[index, articleBody_sentiment_columns] = probs_np\n",
    "\n",
    "        if index > 10:\n",
    "            break\n",
    "            \n",
    "    return _news\n",
    "            \n",
    "\n",
    "news_path = \"/Users/simondanielsson/Documents/Theses/longformer-sentiment/data/totalEURUSDnews.xlsx\"  # change\n",
    "news = pd.read_excel(news_path)\n",
    "\n",
    "news_sentiment = populate_articlebody_sentiment(news, model, tokenizer)\n",
    "\n",
    "save_path = \"../data/news_with_articlebody_sentiment.csv\"  # change\n",
    "news_sentiment.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1c722c28-2094-412c-88a9-cdf97ee2d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mood = pd.read_csv(\"/Users/simondanielsson/Documents/Theses/bsc/predictionservices/data/mood.csv\", sep=\"\\t\")  # change\n",
    "\n",
    "articleBody_sentiment_columns = ['articleBody_negative', 'articleBody_neutral', 'articleBody_positive']\n",
    "full_mood = mood.join(news_sentiment[articleBody_sentiment_columns])\n",
    "\n",
    "full_mood_path = \"/Users/simondanielsson/Documents/Theses/bsc/predictionservices/data/full_mood.csv\"  # change\n",
    "full_mood.to_csv(full_mood_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b0e53-b919-4c98-ba41-430b3b10ba04",
   "metadata": {},
   "source": [
    "How to fetch predicted class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8509196-d9ba-4ed3-9d32-145366513771",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {index: name for index, name in enumerate(['negative', 'neutral', 'positive'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9051f30-1e5e-4f67-98f6-e09908f999de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[\n",
    "    tf.argmax(output.logits, 1).numpy()[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db333d9c-8546-4107-9d56-31af088462d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset argument should be a datasets.Dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_inference_set \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1241\u001b[0m, in \u001b[0;36mTFPreTrainedModel.prepare_tf_dataset\u001b[0;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     collate_fn_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset argument should be a datasets.Dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1242\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall)\u001b[38;5;241m.\u001b[39mparameters)\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   1243\u001b[0m model_labels \u001b[38;5;241m=\u001b[39m find_labels(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Dataset argument should be a datasets.Dataset!"
     ]
    }
   ],
   "source": [
    "tf_inference_set = model.prepare_tf_dataset(inference_data, batch_size=16, shuffle=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad5ea9-1438-4661-adaa-721d41c6218a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
